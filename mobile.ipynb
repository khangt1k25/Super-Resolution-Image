{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, hr_shape):\n",
    "        hr_h, hr_w = hr_shape\n",
    "        \n",
    "        self.lr_transformer = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((hr_h//4, hr_h//4), Image.BICUBIC),\n",
    "                    transforms.ToTensor(), \n",
    "                    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "                ]\n",
    "        )\n",
    "        \n",
    "        self.hr_transformer = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((hr_h, hr_h), Image.BICUBIC),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])\n",
    "                ]\n",
    "        )\n",
    "        \n",
    "        self.files = sorted(glob.glob(root+'/*.*'))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        \n",
    "        lr = self.lr_transformer(img)\n",
    "        hr = self.hr_transformer(img)\n",
    "        \n",
    "        return  {'lr':lr, 'hr':hr}\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voc2012 = ImageDataset('./train/VOC-2012-train/', (88,88))\n",
    "voc2012_test = ImageDataset('./train/VOC-2012-valid/',(88,88))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc2012_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc2012_test = [voc2012_test[i] for i in range(len(voc2012_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./compress_data/test_data.pkl','wb') as f:\n",
    "    pickle.dump(voc2012_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(voc2012, \n",
    "                                         batch_size=32, \n",
    "                                         shuffle=True, \n",
    "                                         num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./compress_data/train_dataloader.pkl','wb') as f:\n",
    "    pickle.dump(dataloader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./compress_data/train_dataset.pkl','rb') as f:\n",
    "    loaded_voc2012 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': tensor([[[-0.0588,  0.0667,  0.2078,  ..., -0.1059, -0.7569, -0.6627],\n",
       "          [ 0.0196,  0.1294,  0.2706,  ..., -0.4118, -0.7098, -0.6784],\n",
       "          [-0.0039,  0.1216,  0.1686,  ..., -0.2549, -0.7098, -0.6549],\n",
       "          ...,\n",
       "          [-0.0118,  0.1451,  0.2392,  ..., -0.9137, -0.9059, -0.8980],\n",
       "          [-0.3255,  0.0118,  0.1451,  ..., -0.9216, -0.9137, -0.8824],\n",
       "          [-0.7647, -0.2000,  0.0510,  ..., -0.9373, -0.9216, -0.8824]],\n",
       " \n",
       "         [[-0.2706, -0.2863, -0.1922,  ..., -0.3098, -0.8431, -0.7804],\n",
       "          [-0.2235, -0.1843, -0.0510,  ..., -0.5765, -0.8039, -0.7804],\n",
       "          [-0.2157, -0.1686, -0.1529,  ..., -0.4667, -0.7961, -0.7725],\n",
       "          ...,\n",
       "          [-0.0353,  0.0824,  0.1608,  ..., -0.8824, -0.8902, -0.8824],\n",
       "          [-0.3490, -0.0431,  0.0588,  ..., -0.8980, -0.8980, -0.8588],\n",
       "          [-0.7647, -0.2471, -0.0196,  ..., -0.9137, -0.9059, -0.8667]],\n",
       " \n",
       "         [[-0.3882, -0.4667, -0.4118,  ..., -0.4510, -0.9294, -0.8902],\n",
       "          [-0.3176, -0.2863, -0.1686,  ..., -0.6863, -0.8902, -0.8745],\n",
       "          [-0.2706, -0.2627, -0.2863,  ..., -0.5922, -0.8902, -0.8667],\n",
       "          ...,\n",
       "          [-0.0431,  0.0588,  0.1294,  ..., -0.8902, -0.8902, -0.8980],\n",
       "          [-0.3647, -0.0588,  0.0196,  ..., -0.8980, -0.9059, -0.8745],\n",
       "          [-0.8039, -0.2706, -0.0588,  ..., -0.9137, -0.9137, -0.8745]]]),\n",
       " 'hr': tensor([[[-0.1373, -0.1686, -0.1294,  ..., -0.6627, -0.7098, -0.6314],\n",
       "          [-0.1529, -0.1216, -0.0667,  ..., -0.6706, -0.6392, -0.6078],\n",
       "          [-0.1373, -0.0980, -0.0196,  ..., -0.6784, -0.6706, -0.6078],\n",
       "          ...,\n",
       "          [-0.7961, -0.8118, -0.8588,  ..., -0.8667, -0.8667, -0.9216],\n",
       "          [-0.7961, -0.7961, -0.8196,  ..., -0.8745, -0.8667, -0.9059],\n",
       "          [-0.7882, -0.7882, -0.7804,  ..., -0.8824, -0.8588, -0.9137]],\n",
       " \n",
       "         [[-0.3412, -0.3725, -0.3176,  ..., -0.7804, -0.8118, -0.7882],\n",
       "          [-0.3333, -0.3020, -0.2471,  ..., -0.7882, -0.7804, -0.7725],\n",
       "          [-0.2706, -0.2627, -0.2078,  ..., -0.8039, -0.7804, -0.7569],\n",
       "          ...,\n",
       "          [-0.7882, -0.8118, -0.8431,  ..., -0.8353, -0.8431, -0.9216],\n",
       "          [-0.7882, -0.7882, -0.8118,  ..., -0.8510, -0.8431, -0.9059],\n",
       "          [-0.7882, -0.7882, -0.7804,  ..., -0.8510, -0.8353, -0.9059]],\n",
       " \n",
       "         [[-0.4667, -0.4824, -0.4275,  ..., -0.8824, -0.9059, -0.9137],\n",
       "          [-0.4431, -0.3725, -0.3490,  ..., -0.8980, -0.8980, -0.8980],\n",
       "          [-0.3333, -0.3333, -0.3020,  ..., -0.9059, -0.8980, -0.8824],\n",
       "          ...,\n",
       "          [-0.8431, -0.8510, -0.8902,  ..., -0.8431, -0.8510, -0.9373],\n",
       "          [-0.8510, -0.8431, -0.8588,  ..., -0.8588, -0.8510, -0.9294],\n",
       "          [-0.8431, -0.8431, -0.8431,  ..., -0.8588, -0.8510, -0.9294]]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_voc2012[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Block(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Residual_Block, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels, 0.8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels, 0.8)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsampling_Block(nn.Module):\n",
    "    def __init__(self, in_channels, up_scale):\n",
    "        super(Upsampling_Block, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels * up_scale** 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(in_channels * up_scale**2),\n",
    "            nn.PixelShuffle(upscale_factor=up_scale),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels = 3, n_residual_blocks = 16, up_scale = 4):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.num_upsample_block = int(math.log(up_scale, 2))\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        res_blocks = []\n",
    "        for _ in range(n_residual_blocks):\n",
    "            res_blocks.append(Residual_Block(in_channels=64))\n",
    "        self.residual_blocks = nn.Sequential(*res_blocks) \n",
    "        \n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        upsampling = []\n",
    "        for i in range(self.num_upsample_block):\n",
    "            upsampling.append(Upsampling_Block(in_channels=64, up_scale=2))          \n",
    "        self.upsampling = nn.Sequential(*upsampling)\n",
    "        \n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=in_channels, kernel_size=9, stride=1, padding=4),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = self.block1(x)\n",
    "        \n",
    "        out = self.residual_blocks(out1)\n",
    "        \n",
    "        out2 = self.block2(out)\n",
    "        \n",
    "        out = torch.add(out1, out2)\n",
    "        \n",
    "        \n",
    "        out = self.upsampling(out)\n",
    "        \n",
    "        out = self.block3(out)\n",
    "    \n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "        \n",
    "        def Discriminator_Block(in_c, out_c, first=False):\n",
    "            layers = []\n",
    "            layers.append(nn.Conv2d(in_c, out_c, kernel_size=3, stride=1, padding=1))\n",
    "            \n",
    "            if first == False:\n",
    "                layers.append(nn.BatchNorm2d(out_c))\n",
    "            \n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append(nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_c))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            \n",
    "            return layers\n",
    "        \n",
    "        layers = []\n",
    "        in_c = in_channels\n",
    "        for i, out_c in enumerate([64, 128, 256, 512]):\n",
    "            block_layer = Discriminator_Block(in_c, out_c, first=(i==0))\n",
    "            layers.extend(block_layer)\n",
    "            in_c = out_c\n",
    "        \n",
    "        layers.append(nn.Conv2d(out_c, 1, kernel_size=3, stride=1, padding=1))\n",
    "         \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        print(out.shape)\n",
    "        return out\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(in_channels=3, n_residual_blocks=1, up_scale=4)\n",
    "discriminator = Discriminator(in_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(params=generator.parameters())\n",
    "optimizer_D = torch.optim.Adam(params=discriminator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        Gloss_epoch = 0.\n",
    "        Dloss_epoch = 0.\n",
    "        for batch, data in enumerate(dataloader):\n",
    "            \n",
    "            \n",
    "            lr = Variable(data['lr'])\n",
    "            hr = Variable(data['hr'])\n",
    "            batch_size = lr.shape[0]\n",
    "            \n",
    "            valid = Variable(torch.Tensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "            fake = Variable(torch.Tensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "            \n",
    "            sr = generator(lr)\n",
    "            \n",
    "            # optimize D\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D = adv_loss(discriminator(sr.detach()), fake)+\\\n",
    "                    adv_loss(discriminator(hr), valid)\n",
    "                \n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # optimize G\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_G = adv_loss(discriminator(sr), valid)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            \n",
    "            Gloss_epoch += loss_G.item()\n",
    "            Dloss_epoch += loss_D.item()\n",
    "            \n",
    "        \n",
    "        \n",
    "            break\n",
    "        print(Gloss_epoch)\n",
    "        print(Dloss_epoch)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
